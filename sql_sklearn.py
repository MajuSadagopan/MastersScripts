import sklearn
import numpy


#Global Variables
database = 'region_waterloo'
user = 'postgres'
password ='love2Learn'
host = 'localhost'
port = '5433'


#SQL QUERIES
#tsvector_query="select gid, to_tsvector('simple',"+text_column+"),"+category+", "+text_column+" from "+table_name+" where "+category+" is not Null"
#tstat_query="select array_agg(word) from ts_stat('"+"select to_tsvector(''simple'', "+text_column+") from "+table_name+"')"


#DB Variables
keys_table = 'iht_word_combos'
key_col = 'name'
key_id = 'id'

table_name = 'iht_trained_survey'
text_column = 'comment'
category = 'match_tabl'
ndocs = 3
w_length = 0
r_score = 2



class sk_sql:
    def __init__(self,database,user,password,host,port):
        # Calls to connect to DB and create a cursor
        import word_grid as wg
        
        self.wgrid = wg.wordgrid(database,user,password,host,port)
        self.key_dict = {}
        self.sk_inputs = {}
        self.categories = []

    # Turns two column query to a dictionary
    def query_dict(self,q_results):
        result_list = {}
        for row in q_results:
            result_list[row[0]] = row[1]
        return result_list
    
    # Query on table generated by word_grid scripts, returns keywords and keyword combinations
    def set_keydict(self,table_name, text_column, id_col):
        query = "select " + text_column + ", "+id_col+" from "+table_name+" where ndocs > "+str(ndocs) +" and w_length > " + str(w_length) +" and r_score > " + str(r_score)
        results = self.wgrid.db_query(query)
        key_dict = self.query_dict(results)
        self.key_dict = key_dict

    # Creates a dictionary from a list, object is key list number is the stored value
    def list2dict(self, wlist):
        wdict = {}
        wset = list(set(wlist))
        for num in range(len(wset)):
            wdict[wset[num]]=num
        return wdict

    # Creates a list of numbers from a list of words
    def list2nums(self,wlist):
        wdict = self.list2dict(wlist)
        output = []
        for w in wlist:
            output.append(wdict[w])
        return output

    # Creates a 2d number array using a dictionary of queried keywords and a text string input
    def doc2numgrid(self, doc):
        keys = self.key_dict.keys()
        grid = []
        for key in keys:
            if key in doc:
                grid.append(self.key_dict[key])
            else:
                grid.append(0)
        return grid

    #Creates a 2d text array using a dictionary of queried keywords
    def doc3wgrid(self, tsvector, length):
        keys = self.key_dict.keys()
        grid = {}
        doc = self.all_combos(tsvector,length)
        #print doc
        for key in keys:
            grid['contains({})'.format(key)] = (key in doc)
        return grid

    # Takes a tsvector dictionary and length and creates all combos of given length
    def tsv_combos(self,diction,length):
        keys = diction.keys()
        sent_combos = []
        for key in keys:
            if key + length < keys[-1]:
                word = diction[key]
                for num in range(1,length):
                    word = word+' '+diction[key+num]
                word = word.replace("'","")
                if word not in sent_combos:
                    sent_combos.append(word)
        return sent_combos
        
    # Takes string of tsvectors splits them and creates a dictionary with positional number indxes
    def tsv_dict(self,tsvector):
        sentence_dict={}
        for pair in tsvector.split():
            pair = pair.split(':')
            word = pair[0]
            number = pair[1]
            for num in number.split(','):
                num = int(num)
                sentence_dict[num] = word
        return sentence_dict

    # Takes in a tsvector creates a dictionary then creates all the possible word combinations in the length range
    def all_combos(self,tsvector, length):
        all_sent_combos = []
        ts_diction = self.tsv_dict(tsvector)
        for num in range(1,length+1):
            all_sent_combos += self.tsv_combos(ts_diction,num)
        return all_sent_combos
            
    # Queries the comments table 
    def sklearn_inputs(self,table_name,text_column,category,keys_table,key_col,key_id):
        self.set_keydict(keys_table, key_col, key_id)
        query = "select "+text_column+", "+category+", to_tsvector('simple',"+text_column+") from "+table_name+" where "+category+" is not Null"
        results = self.wgrid.db_query(query)

        comments = []
        wcomments = []
        categories = []
        for row in results:
            #print self.doc3wgrid(row[2],4)
            wcomments.append(self.doc3wgrid(row[2],4))
            comments.append(self.doc2numgrid(row[0]))
            categories.append(row[1])

            #self.wgrid.tsv_dict(row,)
        comments = numpy.array(comments)
        ncategories = numpy.array(self.list2nums(categories))
        wcategories = numpy.array(categories)
        self.categories = categories
        self.sk_inputs = {'text':comments,'cat_no':ncategories,'cat_text':wcategories,'wtext':wcomments}
        return self.sk_inputs


    # Takes in sklearn or nltk model and outputs array of predicted classifications
    def classify_data(self, model):
        predicted = []
        for row in self.sk_inputs['text']:
            predicted.append(model.predict(row))    
        self.sk_inputs['predict'] = predicted

    # Calculates accuracy of results
    def check_accuracy(self):
        return numpy.mean(self.sk_inputs['predict'] == self.sk_inputs['cat_no'])

    # Calls functions to create formatted data and trains the sklearn model    
    def sk_model(self,table_name,text_column,category,keys_table,key_col,key_id,index):
        self.sk_inputs = self.sklearn_inputs(table_name,text_column,category,keys_table,key_col,key_id)
        
        from sklearn import svm
        clf = svm.SVC(gamma=0.001, C=100.)
        clf.fit(self.sk_inputs['text'][:index],self.sk_inputs['cat_no'][:index])
        self.classify_data(clf)
        return clf

    def nltk_model(self,table_name,text_column,category,keys_table,key_col,key_id,index,location_class):
        print "Start nltk model"
        self.sk_inputs = self.sklearn_inputs(table_name,text_column,category,keys_table,key_col,key_id)

        import nltk
        featuresets = [(self.sk_inputs['wtext'][num], location_class == self.sk_inputs['cat_text'][num]) for num in range(len(self.sk_inputs['cat_no']))]
        print ''
        print location_class
        train_set, test_set = featuresets[index:], featuresets[:index]
        classifier = nltk.NaiveBayesClassifier.train(train_set)

        print nltk.classify.accuracy(classifier, test_set)
        classifier.show_most_informative_features(10)

        return classifier
        

        
        

sksql = sk_sql(database,user,password,host,port)
#sk_model = sksql.sk_model(table_name,text_column,category,keys_table,key_col,key_id,60)

nltk_model = sksql.nltk_model(table_name,text_column,category,keys_table,key_col,key_id,80,'k_iht')

for word in set(sksql.categories):
    nltk_model = sksql.nltk_model(table_name,text_column,category,keys_table,key_col,key_id,80,word)


'''
print sk_model.predict(sksql.sk_inputs['text'][65])
print sksql.sk_inputs['predict'][65]
print sksql.sk_inputs['cat_text'][65]
print sksql.check_accuracy()
sk2 = sk_sql(database,user,password,host,port)
sk2m = sk2.sk_model(table_name,text_column,category,keys_table2,key_col,key_id,60)
print ""
print sk2m.predict(sk2.sk_inputs['text'][65])
print sk2.sk_inputs['predict'][65]
print sk2.sk_inputs['cat_text'][65]
print sk2.check_accuracy()
'''
